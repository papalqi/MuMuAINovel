"""å‘é‡è®°å¿†æœåŠ¡ - åŸºäºChromaDBå®ç°é•¿æœŸè®°å¿†å’Œè¯­ä¹‰æ£€ç´¢"""
import asyncio
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
from app.logger import get_logger
import os
import hashlib
import httpx
from urllib.parse import urljoin

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.settings import Settings

logger = get_logger(__name__)

# é…ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
# ä¼˜å…ˆä½¿ç”¨ backend/embedding ç›®å½•ï¼ˆæ‰“åŒ…åçš„å®é™…ä½ç½®ï¼‰
import sys
from pathlib import Path

if 'SENTENCE_TRANSFORMERS_HOME' not in os.environ:
    # æ ¹æ®è¿è¡Œç¯å¢ƒç¡®å®šæ¨¡å‹ç›®å½•
    if getattr(sys, 'frozen', False):
        # PyInstaller æ‰“åŒ…å - éœ€è¦æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
        exe_dir = Path(sys.executable).parent
        
        # æ£€æŸ¥é¡ºåºï¼š
        # 1. _MEIPASS/backend/embedding (ä¸´æ—¶è§£å‹ç›®å½•)
        # 2. exeåŒçº§/_internal/backend/embedding
        # 3. exeåŒçº§/backend/embedding
        possible_paths = []
        
        if hasattr(sys, '_MEIPASS'):
            possible_paths.append(Path(sys._MEIPASS) / 'backend' / 'embedding')
        
        possible_paths.extend([
            exe_dir / '_internal' / 'backend' / 'embedding',
            exe_dir / 'backend' / 'embedding',
            exe_dir / '_internal' / 'embedding',
            exe_dir / 'embedding'
        ])
        
        model_dir = None
        for path in possible_paths:
            if path.exists():
                model_dir = path
                logger.info(f"ğŸ”§ æ‰¾åˆ°æ‰“åŒ…ç¯å¢ƒæ¨¡å‹ç›®å½•: {model_dir}")
                break
        
        if model_dir:
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
        else:
            # æœ€åé™çº§æ–¹æ¡ˆ
            fallback_dir = exe_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„æ‰“åŒ…æ¨¡å‹ï¼Œä½¿ç”¨é™çº§ç›®å½•: {fallback_dir}")
            logger.warning(f"   æ£€æŸ¥è¿‡çš„è·¯å¾„: {[str(p) for p in possible_paths]}")
    else:
        # å¼€å‘æ¨¡å¼ï¼Œä»å½“å‰æ–‡ä»¶ä½ç½®å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        base_dir = Path(__file__).parent.parent.parent
        model_dir = base_dir / 'backend' / 'embedding'
        if model_dir.exists():
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
            logger.info(f"ğŸ”§ è®¾ç½®å¼€å‘ç¯å¢ƒæ¨¡å‹ç›®å½•: {model_dir}")
        else:
            # é™çº§åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ embedding
            fallback_dir = base_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.info(f"ğŸ”§ ä½¿ç”¨é™çº§æ¨¡å‹ç›®å½•: {fallback_dir}")


class MemoryService:
    """å‘é‡è®°å¿†ç®¡ç†æœåŠ¡ - å®ç°è¯­ä¹‰æ£€ç´¢å’Œé•¿æœŸè®°å¿†"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–ChromaDBå’ŒEmbeddingæ¨¡å‹"""
        if self._initialized:
            return
            
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            chroma_dir = "data/chroma_db"
            os.makedirs(chroma_dir, exist_ok=True)
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯(ä½¿ç”¨æ–°API - PersistentClient)
            self.client = chromadb.PersistentClient(path=chroma_dir)
            
            # åˆå§‹åŒ–å¤šè¯­è¨€embeddingæ¨¡å‹(æ”¯æŒä¸­æ–‡)
            logger.info("ğŸ”„ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹...")
            
            # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„æ¨¡å‹ç›®å½•
            model_cache_dir = os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'embedding')
            os.makedirs(model_cache_dir, exist_ok=True)
            logger.info(f"ğŸ“‚ ä½¿ç”¨æ¨¡å‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°ç¯å¢ƒå˜é‡å’Œè·¯å¾„
            logger.info(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            logger.info(f"ğŸ“‚ æ¨¡å‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            logger.info(f"ğŸ”§ SENTENCE_TRANSFORMERS_HOME: {os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'æœªè®¾ç½®')}")
            logger.info(f"ğŸ”§ TRANSFORMERS_OFFLINE: {os.environ.get('TRANSFORMERS_OFFLINE', 'æœªè®¾ç½®')}")
            logger.info(f"ğŸ”§ HF_HUB_OFFLINE: {os.environ.get('HF_HUB_OFFLINE', 'æœªè®¾ç½®')}")
            
            # æ£€æŸ¥æ¨¡å‹ç›®å½•å†…å®¹
            abs_cache_dir = os.path.abspath(model_cache_dir)
            logger.info(f"ğŸ“‚ æ£€æŸ¥æ¨¡å‹ç¼“å­˜ç›®å½•: {abs_cache_dir}")
            
            if os.path.exists(abs_cache_dir):
                logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹...")
                try:
                    items = os.listdir(abs_cache_dir)
                    logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•å†…å®¹ ({len(items)} é¡¹): {items}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„æ¨¡å‹æ–‡ä»¶å¤¹
                    expected_model_dir = os.path.join(abs_cache_dir, 'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info(f"ğŸ” æ£€æŸ¥é¢„æœŸè·¯å¾„: {expected_model_dir}")
                    
                    if os.path.exists(expected_model_dir):
                        logger.info(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•!")
                        # æ£€æŸ¥å¿«ç…§ç›®å½•
                        snapshots_dir = os.path.join(expected_model_dir, 'snapshots')
                        if os.path.exists(snapshots_dir):
                            snapshots = os.listdir(snapshots_dir)
                            logger.info(f"ğŸ“ æ¨¡å‹å¿«ç…§ ({len(snapshots)} ä¸ª): {snapshots}")
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å¿«ç…§
                            if snapshots:
                                logger.info(f"âœ… å‘ç°æœ‰æ•ˆå¿«ç…§ï¼Œå¯ä»¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•")
                        logger.warning(f"   é¢„æœŸä½ç½®: {expected_model_dir}")
                except Exception as e:
                    logger.error(f"âŒ æ£€æŸ¥æ¨¡å‹ç›®å½•å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(f"   å †æ ˆ: {traceback.format_exc()}")
            else:
                logger.warning(f"âš ï¸ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {abs_cache_dir}")
            
            try:
                logger.info("ğŸ”„ å°è¯•åŠ è½½ä¸»æ¨¡å‹: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                
                # ä½¿ç”¨ç»å¯¹è·¯å¾„æ£€æŸ¥æœ¬åœ°æ¨¡å‹
                abs_cache_dir = os.path.abspath(model_cache_dir)
                local_model_path = os.path.join(
                    abs_cache_dir,
                    'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2'
                )
                
                logger.info(f"ğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„: {local_model_path}")
                logger.info(f"ğŸ” è·¯å¾„å­˜åœ¨æ£€æŸ¥: {os.path.exists(local_model_path)}")
                
                # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
                snapshots_dir = os.path.join(local_model_path, 'snapshots')
                has_valid_model = False
                if os.path.exists(snapshots_dir):
                    try:
                        snapshots = os.listdir(snapshots_dir)
                        if snapshots:
                            logger.info(f"âœ… å‘ç°æœ¬åœ°æ¨¡å‹å¿«ç…§: {snapshots}")
                            has_valid_model = True
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ£€æŸ¥å¿«ç…§å¤±è´¥: {e}")
                
                # ä¼˜å…ˆå°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½
                if has_valid_model:
                    logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´æœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼åŠ è½½")
                    try:
                        self.embedding_model = SentenceTransformer(
                            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                            cache_folder=abs_cache_dir,
                            device='cpu',
                            trust_remote_code=True,
                            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                        )
                        logger.info("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (ç¦»çº¿æ¨¡å¼)")
                    except Exception as local_err:
                        logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {str(local_err)}")
                        logger.info("ğŸ”„ å°è¯•åœ¨çº¿æ¨¡å¼...")
                        raise local_err
                else:
                    logger.info("ğŸ“¥ æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œå°†è”ç½‘ä¸‹è½½...")
                    logger.info(f"   ä¸‹è½½åå°†ä¿å­˜åˆ°: {abs_cache_dir}")
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        cache_folder=abs_cache_dir,
                        device='cpu',
                        trust_remote_code=True,
                        local_files_only=False  # å…è®¸è”ç½‘ä¸‹è½½
                    )
                    logger.info("âœ… Embeddingæ¨¡å‹åŠ è½½æˆåŠŸ (åœ¨çº¿ä¸‹è½½)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å¤šè¯­è¨€æ¨¡å‹: {str(e)}")
                logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e)}")
                import traceback
                logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                logger.info("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹: sentence-transformers/all-MiniLM-L6-v2")
                try:
                    # é™çº§åˆ°æ›´å°çš„æ¨¡å‹ä½œä¸ºå¤‡é€‰
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/all-MiniLM-L6-v2',
                        cache_folder=model_cache_dir,
                        device='cpu',
                        trust_remote_code=False
                    )
                    logger.info("âœ… ä½¿ç”¨å¤‡ç”¨Embeddingæ¨¡å‹ (all-MiniLM-L6-v2)")
                except Exception as e2:
                    logger.error(f"âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥: {str(e2)}")
                    logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e2)}")
                    import traceback
                    logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                    logger.error("ğŸ’¡ æ¨¡å‹é¦–æ¬¡ä½¿ç”¨éœ€è¦è”ç½‘ä¸‹è½½ï¼ˆçº¦420MBï¼‰")
                    logger.error("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ° embedding ç›®å½•")
                    logger.error(f"ğŸ’¡ æœŸæœ›çš„æ¨¡å‹ç›®å½•ç»“æ„:")
                    logger.error(f"   {os.path.abspath(model_cache_dir)}/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/")
                    # âš ï¸ ä¸å†é˜»æ–­æœåŠ¡å¯åŠ¨ï¼š
                    # - å¦‚æœç”¨æˆ·é€‰æ‹©â€œè¿œç«¯ Embeddingâ€ï¼Œæœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ä¹Ÿå¯ä»¥å·¥ä½œ
                    # - è‹¥åç»­ä»ä½¿ç”¨æœ¬åœ° embeddingï¼Œä¼šåœ¨è°ƒç”¨æ—¶æŠ›é”™å¹¶æç¤º
                    self.embedding_model = None
                    logger.warning("âš ï¸ MemoryService å°†ä»¥â€œæ— æœ¬åœ°Embeddingæ¨¡å‹â€æ¨¡å¼ç»§ç»­è¿è¡Œï¼ˆå¯é…ç½®è¿œç«¯Embeddingï¼‰")
            
            self._initialized = True
            logger.info("âœ… MemoryServiceåˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"  - ChromaDBç›®å½•: {chroma_dir}")
            logger.info(
                "  - Embeddingæ¨¡å‹: "
                + ("paraphrase-multilingual-MiniLM-L12-v2" if self.embedding_model else "æœªåŠ è½½ï¼ˆå¯ä½¿ç”¨è¿œç«¯Embeddingï¼‰")
            )
            
        except Exception as e:
            logger.error(f"âŒ MemoryServiceåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def get_collection(self, user_id: str, project_id: str, embed_id: str = "local"):
        """
        è·å–æˆ–åˆ›å»ºé¡¹ç›®çš„è®°å¿†é›†åˆ
        
        æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªé¡¹ç›®æœ‰ç‹¬ç«‹çš„collection,å®ç°æ•°æ®éš”ç¦»
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            ChromaDB Collectionå¯¹è±¡
        """
        # ChromaDB collectionå‘½åè§„åˆ™ï¼š
        # 1. 3-63å­—ç¬¦ï¼ˆæœ€é‡è¦ï¼ï¼‰
        # 2. å¼€å¤´å’Œç»“å°¾å¿…é¡»æ˜¯å­—æ¯æˆ–æ•°å­—
        # 3. åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿æˆ–çŸ­æ¨ªçº¿
        # 4. ä¸èƒ½åŒ…å«è¿ç»­çš„ç‚¹(..)
        # 5. ä¸èƒ½æ˜¯æœ‰æ•ˆçš„IPv4åœ°å€
        
        # ä½¿ç”¨SHA256å“ˆå¸Œå‹ç¼©IDé•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡63å­—ç¬¦ã€‚
        # åŒä¸€ user+project åœ¨ä¸åŒ embedding é…ç½®ä¸‹éœ€è¦ä½¿ç”¨ä¸åŒ collectionï¼Œ
        # å¦åˆ™ä¼šå‡ºç°å‘é‡ç»´åº¦ä¸ä¸€è‡´çš„é—®é¢˜ã€‚
        #
        # æ ¼å¼:
        # - æ—§ç‰ˆï¼ˆæœ¬åœ° embeddingï¼Œå‘åå…¼å®¹ï¼‰: u_{user_hash}_p_{project_hash}
        # - æ–°ç‰ˆï¼ˆè¿œç«¯ embeddingï¼‰: u_{user_hash}_p_{project_hash}_e_{embed_hash}
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
        embed_id_norm = str(embed_id or "local")

        if embed_id_norm in ("local", "default"):
            collection_name = f"u_{user_hash}_p_{project_hash}"
        else:
            embed_hash = hashlib.sha256(embed_id_norm.encode()).hexdigest()[:8]
            collection_name = f"u_{user_hash}_p_{project_hash}_e_{embed_hash}"
        
        try:
            return self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "user_id": user_id,
                    "project_id": project_id,
                    "embed_id": embed_id_norm[:200],
                    "created_at": datetime.now().isoformat()
                }
            )
        except Exception as e:
            logger.error(f"âŒ è·å–collectionå¤±è´¥: {str(e)}")
            raise

    # ==================== Retrieval Settings / Remote Backend ====================

    @staticmethod
    def _safe_load_prefs(preferences: Optional[str]) -> Dict[str, Any]:
        if not preferences:
            return {}
        try:
            raw = json.loads(preferences)
            return raw if isinstance(raw, dict) else {}
        except json.JSONDecodeError:
            return {}

    @staticmethod
    def _collection_prefix(user_id: str, project_id: str) -> str:
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
        return f"u_{user_hash}_p_{project_hash}"

    def _list_project_collection_names(self, user_id: str, project_id: str) -> List[str]:
        """
        åˆ—å‡ºå½“å‰ user+project å¯¹åº”çš„æ‰€æœ‰ collectionï¼ˆåŒ…å«æ—§ç‰ˆ local ä¸æ–°ç‰ˆ remoteï¼‰ã€‚
        ç”¨äºæ¸…ç†åœºæ™¯ï¼ˆåˆ é™¤ç« èŠ‚ã€åˆ é™¤é¡¹ç›®ç­‰ï¼‰ã€‚
        """
        prefix = self._collection_prefix(user_id, project_id)
        try:
            cols = self.client.list_collections()
            names: List[str] = []
            for c in cols or []:
                name = getattr(c, "name", None) or (c.get("name") if isinstance(c, dict) else None)
                if not name:
                    continue
                if name == prefix or str(name).startswith(prefix + "_e_"):
                    names.append(str(name))
            return names
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ—å‡º collection å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨é»˜è®¤ collection: {e}")
            return [prefix]

    async def _get_user_settings_and_retrieval(
        self,
        user_id: str,
        db: Optional[AsyncSession],
    ) -> tuple[Optional[Settings], Dict[str, Any]]:
        """
        è¿”å› (Settingså¯¹è±¡, retrievalé…ç½®dict)ã€‚db ä¸ºç©ºæ—¶è¿”å› (None, {})ã€‚
        """
        if not db or not user_id:
            return None, {}

        try:
            result = await db.execute(select(Settings).where(Settings.user_id == user_id))
            settings = result.scalar_one_or_none()
            prefs = self._safe_load_prefs(settings.preferences if settings else None)
            retrieval = prefs.get("retrieval") if isinstance(prefs.get("retrieval"), dict) else {}
            return settings, (retrieval if isinstance(retrieval, dict) else {})
        except Exception as e:
            logger.warning(f"âš ï¸ è¯»å–ç”¨æˆ·æ£€ç´¢é…ç½®å¤±è´¥ï¼Œå›é€€æœ¬åœ° embedding: {e}")
            return None, {}

    @staticmethod
    def _resolve_embedding_backend(
        retrieval: Dict[str, Any],
        settings: Optional[Settings],
    ) -> Dict[str, Any]:
        """
        è§£æ embedding é…ç½®ã€‚

        è¿”å›ï¼š
        - backend: local | remote
        - embed_id: ç”¨äº collection éš”ç¦»
        - (remote é¢å¤–å­—æ®µ): provider/api_key/api_base_url/model/timeout_s
        """
        embedding_cfg = retrieval.get("embedding") if isinstance(retrieval.get("embedding"), dict) else {}
        backend = str(embedding_cfg.get("backend") or "local").lower()

        if backend == "remote":
            remote = embedding_cfg.get("remote") if isinstance(embedding_cfg.get("remote"), dict) else {}
            provider = str(remote.get("provider") or "openai_compatible")
            model = remote.get("model")
            api_base_url = remote.get("api_base_url") or (settings.api_base_url if settings else None)
            api_key = remote.get("api_key") or (settings.api_key if settings else None)
            timeout_s = int(remote.get("timeout_s") or 60)

            if api_base_url and model:
                embed_id = f"remote:{provider}:{api_base_url}:{model}"
                return {
                    "backend": "remote",
                    "embed_id": embed_id,
                    "provider": provider,
                    "api_key": api_key,
                    "api_base_url": api_base_url,
                    "model": model,
                    "timeout_s": timeout_s,
                }

        # fallbackï¼šæœ¬åœ°
        return {"backend": "local", "embed_id": "local"}

    @staticmethod
    def _resolve_rerank_backend(
        retrieval: Dict[str, Any],
        settings: Optional[Settings],
    ) -> Dict[str, Any]:
        rerank_cfg = retrieval.get("rerank") if isinstance(retrieval.get("rerank"), dict) else {}
        enabled = bool(rerank_cfg.get("enabled"))
        if not enabled:
            return {"enabled": False}

        remote = rerank_cfg.get("remote") if isinstance(rerank_cfg.get("remote"), dict) else {}
        provider = str(remote.get("provider") or "cohere_compatible")
        model = remote.get("model")
        api_base_url = remote.get("api_base_url") or (settings.api_base_url if settings else None)
        api_key = remote.get("api_key") or (settings.api_key if settings else None)
        timeout_s = int(remote.get("timeout_s") or 60)
        top_k = int(remote.get("top_k") or 30)
        top_n = int(remote.get("top_n") or 10)
        min_score = remote.get("min_score")
        try:
            min_score = float(min_score) if min_score is not None else None
        except Exception:
            min_score = None

        if api_base_url and model:
            return {
                "enabled": True,
                "provider": provider,
                "api_key": api_key,
                "api_base_url": api_base_url,
                "model": model,
                "timeout_s": timeout_s,
                "top_k": max(1, top_k),
                "top_n": max(1, top_n),
                "min_score": min_score,
            }

        # é…ç½®ä¸å®Œæ•´åˆ™ç¦ç”¨
        return {"enabled": False}

    @staticmethod
    def _build_openai_compatible_url(api_base_url: str, endpoint: str) -> str:
        """
        å°†å½¢å¦‚ https://host/v1 + embeddings -> https://host/v1/embeddings
        """
        base = (api_base_url or "").rstrip("/")
        if base.endswith("/" + endpoint.strip("/")):
            return base
        return urljoin(base + "/", endpoint.lstrip("/"))

    async def _embed_texts(
        self,
        texts: List[str],
        embed_backend: Dict[str, Any],
    ) -> List[List[float]]:
        backend = embed_backend.get("backend") or "local"

        if backend == "remote":
            provider = embed_backend.get("provider")
            if provider != "openai_compatible":
                # å½“å‰ä»…å†…ç½® OpenAI å…¼å®¹ embeddingï¼›å…¶ä»– provider å…ˆæŒ‰å…¼å®¹å¤„ç†
                provider = "openai_compatible"
            return await self._embed_texts_remote_openai_compatible(
                api_base_url=str(embed_backend.get("api_base_url") or ""),
                api_key=str(embed_backend.get("api_key") or ""),
                model=str(embed_backend.get("model") or ""),
                texts=texts,
                timeout_s=int(embed_backend.get("timeout_s") or 60),
            )

        # local
        if not self.embedding_model:
            raise RuntimeError("æœ¬åœ°Embeddingæ¨¡å‹æœªåŠ è½½ï¼šè¯·æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼Œæˆ–åœ¨è®¾ç½®ä¸­å¯ç”¨è¿œç«¯Embeddingã€‚")

        # sentence-transformers å¯¹ list è¾“å…¥è¿”å› List[List[float]]
        vectors = await asyncio.to_thread(self.embedding_model.encode, texts)
        return vectors.tolist() if hasattr(vectors, "tolist") else vectors

    async def _embed_texts_remote_openai_compatible(
        self,
        *,
        api_base_url: str,
        api_key: str,
        model: str,
        texts: List[str],
        timeout_s: int = 60,
        batch_size: int = 64,
    ) -> List[List[float]]:
        if not api_base_url or not model:
            raise ValueError("è¿œç«¯Embeddingé…ç½®ä¸å®Œæ•´ï¼šapi_base_url / model ä¸èƒ½ä¸ºç©º")

        url = self._build_openai_compatible_url(api_base_url, "embeddings")

        headers: Dict[str, str] = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        embeddings: List[List[float]] = []

        async with httpx.AsyncClient(timeout=timeout_s) as client:
            for i in range(0, len(texts), batch_size):
                chunk = texts[i:i + batch_size]
                # éƒ¨åˆ† OpenAI-å…¼å®¹æœåŠ¡ï¼ˆä¾‹å¦‚ ModelScopeï¼‰è¦æ±‚æ˜¾å¼ä¼ å…¥ encoding_format
                payload = {"model": model, "input": chunk, "encoding_format": "float"}
                resp = await client.post(url, headers=headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                items = data.get("data") or []
                # OpenAI æ ¼å¼ï¼šdata=[{index, embedding, ...}]
                try:
                    items = sorted(items, key=lambda x: int(x.get("index", 0)))
                except Exception:
                    pass
                for item in items:
                    emb = item.get("embedding")
                    if not isinstance(emb, list):
                        raise RuntimeError("è¿œç«¯Embeddingè¿”å›æ ¼å¼å¼‚å¸¸ï¼šembedding ä¸æ˜¯ list")
                    embeddings.append(emb)

        if len(embeddings) != len(texts):
            raise RuntimeError(f"è¿œç«¯Embeddingè¿”å›æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ›{len(texts)}ï¼Œå®é™…{len(embeddings)}")

        return embeddings

    async def _rerank_remote_cohere_compatible(
        self,
        *,
        api_base_url: str,
        api_key: str,
        model: str,
        query: str,
        documents: List[str],
        top_n: int,
        timeout_s: int = 60,
        min_score: Optional[float] = None,
    ) -> List[Dict[str, Any]]:
        """
        Cohere å…¼å®¹ rerankï¼š
        è¯·æ±‚ï¼š{model, query, documents, top_n}
        å“åº”ï¼š{results:[{index, relevance_score}, ...]}
        """
        if not api_base_url or not model:
            raise ValueError("è¿œç«¯Reranké…ç½®ä¸å®Œæ•´ï¼šapi_base_url / model ä¸èƒ½ä¸ºç©º")

        url = self._build_openai_compatible_url(api_base_url, "rerank")

        headers: Dict[str, str] = {"Content-Type": "application/json"}
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        payload = {
            "model": model,
            "query": query,
            "documents": documents,
            "top_n": top_n,
        }

        async with httpx.AsyncClient(timeout=timeout_s) as client:
            resp = await client.post(url, headers=headers, json=payload)
            resp.raise_for_status()
            data = resp.json()

        results = data.get("results") or data.get("data") or []
        formatted: List[Dict[str, Any]] = []
        for r in results:
            try:
                idx = int(r.get("index"))
            except Exception:
                continue
            score = r.get("relevance_score", r.get("score"))
            try:
                score_f = float(score) if score is not None else None
            except Exception:
                score_f = None

            if min_score is not None and score_f is not None and score_f < min_score:
                continue

            formatted.append({"index": idx, "score": score_f})

        # æŒ‰åˆ†æ•°æ’åºï¼ˆé«˜->ä½ï¼‰ï¼›è‹¥æ— åˆ†æ•°åˆ™ä¿æŒåŸæ ·
        formatted.sort(key=lambda x: (x["score"] is not None, x["score"]), reverse=True)
        return formatted
    
    async def add_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: str,
        memory_type: str,
        metadata: Dict[str, Any],
        db: Optional[AsyncSession] = None,
    ) -> bool:
        """
        æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†å”¯ä¸€ID
            content: è®°å¿†å†…å®¹(å°†è¢«è½¬æ¢ä¸ºå‘é‡)
            memory_type: è®°å¿†ç±»å‹
            metadata: é™„åŠ å…ƒæ•°æ®
        
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))

            # ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤ºï¼ˆæœ¬åœ°æˆ–è¿œç«¯ï¼‰
            embedding = (await self._embed_texts([content], embed_backend))[0]
            
            # å‡†å¤‡å…ƒæ•°æ®(ChromaDBè¦æ±‚æ‰€æœ‰å€¼ä¸ºåŸºç¡€ç±»å‹)
            chroma_metadata = {
                "memory_type": memory_type,
                "chapter_id": str(metadata.get("chapter_id", "")),
                "chapter_number": int(metadata.get("chapter_number", 0)),
                "importance": float(metadata.get("importance_score", 0.5)),
                "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                "title": str(metadata.get("title", ""))[:200],  # é™åˆ¶é•¿åº¦
                "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ ç›¸å…³è§’è‰²ä¿¡æ¯
            if metadata.get("related_characters"):
                chroma_metadata["related_characters"] = json.dumps(
                    metadata["related_characters"], 
                    ensure_ascii=False
                )
            
            # å­˜å‚¨åˆ°å‘é‡åº“
            collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[content],
                metadatas=[chroma_metadata]
            )
            
            logger.info(f"âœ… è®°å¿†å·²æ·»åŠ : {memory_id[:8]}... (ç±»å‹:{memory_type}, é‡è¦æ€§:{chroma_metadata['importance']})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def batch_add_memories(
        self,
        user_id: str,
        project_id: str,
        memories: List[Dict[str, Any]],
        db: Optional[AsyncSession] = None,
    ) -> int:
        """
        æ‰¹é‡æ·»åŠ è®°å¿†(æ€§èƒ½æ›´å¥½)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memories: è®°å¿†åˆ—è¡¨,æ¯ä¸ªåŒ…å«idã€contentã€typeã€metadata
        
        Returns:
            æˆåŠŸæ·»åŠ çš„æ•°é‡
        """
        if not memories:
            return 0
            
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))
            
            ids = []
            documents = []
            metadatas = []
            
            # æ‰¹é‡å‡†å¤‡æ•°æ®
            for mem in memories:
                ids.append(mem['id'])
                documents.append(mem['content'])
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = mem.get('metadata', {})
                chroma_metadata = {
                    "memory_type": mem['type'],
                    "chapter_id": str(metadata.get("chapter_id", "")),
                    "chapter_number": int(metadata.get("chapter_number", 0)),
                    "importance": float(metadata.get("importance_score", 0.5)),
                    "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                    "title": str(metadata.get("title", ""))[:200],
                    "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                    "created_at": datetime.now().isoformat()
                }
                metadatas.append(chroma_metadata)

            # æ‰¹é‡ç”Ÿæˆ embeddingï¼ˆæœ¬åœ°æˆ–è¿œç«¯ï¼‰
            embeddings = await self._embed_texts(documents, embed_backend)
            
            # æ‰¹é‡æ·»åŠ 
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            logger.info(f"âœ… æ‰¹é‡æ·»åŠ è®°å¿†æˆåŠŸ: {len(memories)}æ¡")
            return len(memories)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return 0
    
    async def search_memories(
        self,
        user_id: str,
        project_id: str,
        query: str,
        memory_types: Optional[List[str]] = None,
        limit: int = 10,
        min_importance: float = 0.0,
        chapter_range: Optional[tuple] = None,
        db: Optional[AsyncSession] = None,
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            query: æŸ¥è¯¢æ–‡æœ¬(ä¼šè¢«è½¬æ¢ä¸ºå‘é‡è¿›è¡Œç›¸ä¼¼åº¦æœç´¢)
            memory_types: è¿‡æ»¤ç‰¹å®šç±»å‹çš„è®°å¿†
            limit: è¿”å›ç»“æœæ•°é‡
            min_importance: æœ€ä½é‡è¦æ€§é˜ˆå€¼
            chapter_range: ç« èŠ‚èŒƒå›´ (start, end)
        
        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨,æŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            rerank_backend = self._resolve_rerank_backend(retrieval, settings)

            # rerank éœ€è¦æ›´å¤§çš„å€™é€‰é›†åˆ
            candidate_limit = int(limit or 10)
            if rerank_backend.get("enabled"):
                candidate_limit = max(candidate_limit, int(rerank_backend.get("top_k") or candidate_limit))

            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))

            # ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆæœ¬åœ°æˆ–è¿œç«¯ï¼‰
            query_embedding = (await self._embed_texts([query], embed_backend))[0]
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶ - ChromaDBè¦æ±‚ä½¿ç”¨$andç»„åˆå¤šä¸ªæ¡ä»¶
            where_filter = None
            conditions = []
            
            if memory_types:
                conditions.append({"memory_type": {"$in": memory_types}})
            if min_importance > 0:
                conditions.append({"importance": {"$gte": min_importance}})
            if chapter_range:
                conditions.append({"chapter_number": {"$gte": chapter_range[0]}})
                conditions.append({"chapter_number": {"$lte": chapter_range[1]}})
            
            # æ ¹æ®æ¡ä»¶æ•°é‡é€‰æ‹©åˆé€‚çš„æ ¼å¼
            if len(conditions) == 0:
                where_filter = None
            elif len(conditions) == 1:
                where_filter = conditions[0]
            else:
                where_filter = {"$and": conditions}
            
            # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=candidate_limit,
                where=where_filter
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            memories = []
            if results['ids'] and results['ids'][0]:
                for i in range(len(results['ids'][0])):
                    memories.append({
                        "id": results['ids'][0][i],
                        "content": results['documents'][0][i],
                        "metadata": results['metadatas'][0][i],
                        "similarity": 1 - results['distances'][0][i] if 'distances' in results else 1.0,
                        "distance": results['distances'][0][i] if 'distances' in results else 0.0
                    })
            
            logger.info(f"ğŸ” è¯­ä¹‰æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:30]}...', æ‰¾åˆ°{len(memories)}æ¡è®°å¿†")
            # è¿œç«¯ rerankï¼ˆå¯é€‰ï¼‰
            if rerank_backend.get("enabled") and memories:
                try:
                    # rerank è¾“å…¥è¿‡é•¿ä¼šå¯¼è‡´æˆæœ¬/å»¶è¿Ÿé™¡å¢ï¼Œè¿™é‡Œåšä¸€ä¸ªè½»é‡æˆªæ–­
                    docs_for_rerank = [
                        (m.get("content", "") or "")[:512]
                        for m in memories
                    ]
                    top_n_for_rerank = max(
                        1,
                        min(len(docs_for_rerank), max(int(limit or 10), int(rerank_backend.get("top_n") or 10)))
                    )
                    rr = await self._rerank_remote_cohere_compatible(
                        api_base_url=str(rerank_backend.get("api_base_url") or ""),
                        api_key=str(rerank_backend.get("api_key") or ""),
                        model=str(rerank_backend.get("model") or ""),
                        query=query,
                        documents=docs_for_rerank,
                        top_n=top_n_for_rerank,
                        timeout_s=int(rerank_backend.get("timeout_s") or 60),
                        min_score=rerank_backend.get("min_score"),
                    )
                    if rr:
                        reordered: List[Dict[str, Any]] = []
                        used = set()
                        for item in rr:
                            idx = item.get("index")
                            if idx is None:
                                continue
                            if not isinstance(idx, int):
                                try:
                                    idx = int(idx)
                                except Exception:
                                    continue
                            if idx < 0 or idx >= len(memories):
                                continue
                            mem = dict(memories[idx])
                            mem["rerank_score"] = item.get("score")
                            reordered.append(mem)
                            used.add(idx)

                        # å°†æœªå‘½ä¸­çš„å€™é€‰æŒ‰åŸå‘é‡æ’åºè¿½åŠ ï¼ˆç”¨äº top_n < candidate_limit åœºæ™¯ï¼‰
                        for i, m in enumerate(memories):
                            if i in used:
                                continue
                            reordered.append(m)

                        memories = reordered
                        logger.info(
                            f"ğŸ” rerank ç”Ÿæ•ˆ: candidates={len(docs_for_rerank)}, return_top_n={top_n_for_rerank}, final_limit={min(len(memories), int(limit or 10))}"
                        )
                except Exception as e:
                    logger.warning(f"âš ï¸ rerank å¤±è´¥ï¼Œå›é€€å‘é‡ç›¸ä¼¼åº¦æ’åº: {e}")

            # æŒ‰è°ƒç”¨æ–¹ limit æˆªæ–­
            return memories[: int(limit or 10)]
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def get_recent_memories(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        recent_count: int = 3,
        min_importance: float = 0.5,
        db: Optional[AsyncSession] = None,
    ) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘å‡ ç« çš„é‡è¦è®°å¿†(ç”¨äºä¿æŒè¿è´¯æ€§)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            recent_count: è·å–æœ€è¿‘å‡ ç« 
            min_importance: æœ€ä½é‡è¦æ€§é˜ˆå€¼
        
        Returns:
            æœ€è¿‘ç« èŠ‚çš„è®°å¿†åˆ—è¡¨,æŒ‰é‡è¦æ€§æ’åº
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))
            
            # è®¡ç®—ç« èŠ‚èŒƒå›´
            start_chapter = max(1, current_chapter - recent_count)
            
            # è·å–æœ€è¿‘ç« èŠ‚çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"chapter_number": {"$gte": start_chapter}},
                        {"chapter_number": {"$lt": current_chapter}},
                        {"importance": {"$gte": min_importance}}
                    ]
                },
                limit=100  # å…ˆè·å–è¶³å¤Ÿå¤šçš„è®°å¿†
            )
            
            memories = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    memories.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§å’Œç« èŠ‚å·æ’åº
            memories.sort(
                key=lambda x: (float(x['metadata'].get('importance', 0)), 
                              int(x['metadata'].get('chapter_number', 0))),
                reverse=True
            )
            
            # è¿”å›æœ€é‡è¦çš„å‰Næ¡
            top_memories = memories[:20]
            logger.info(f"ğŸ“š è·å–æœ€è¿‘è®°å¿†: ç« èŠ‚{start_chapter}-{current_chapter-1}, æ‰¾åˆ°{len(top_memories)}æ¡")
            return top_memories
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€è¿‘è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def find_unresolved_foreshadows(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        db: Optional[AsyncSession] = None,
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æœªå®Œç»“çš„ä¼ç¬”
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
        
        Returns:
            æœªå®Œç»“ä¼ç¬”åˆ—è¡¨
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))
            
            # æŸ¥æ‰¾ä¼ç¬”çŠ¶æ€ä¸º1(å·²åŸ‹ä¸‹ä½†æœªå›æ”¶)çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"is_foreshadow": 1},
                        {"chapter_number": {"$lt": current_chapter}}
                    ]
                },
                limit=50
            )
            
            foreshadows = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    foreshadows.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§æ’åº
            foreshadows.sort(
                key=lambda x: float(x['metadata'].get('importance', 0)),
                reverse=True
            )
            
            logger.info(f"ğŸ£ æ‰¾åˆ°æœªå®Œç»“ä¼ç¬”: {len(foreshadows)}ä¸ª")
            return foreshadows
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾ä¼ç¬”å¤±è´¥: {str(e)}")
            return []
    
    async def build_context_for_generation(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        chapter_outline: str,
        character_names: List[str] = None,
        db: Optional[AsyncSession] = None,
    ) -> Dict[str, Any]:
        """
        ä¸ºç« èŠ‚ç”Ÿæˆæ„å»ºæ™ºèƒ½ä¸Šä¸‹æ–‡
        
        è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½: ç»“åˆå¤šç§æ£€ç´¢ç­–ç•¥,ä¸ºAIç”Ÿæˆæä¾›æœ€ç›¸å…³çš„è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            chapter_outline: æœ¬ç« å¤§çº²
            character_names: æ¶‰åŠçš„è§’è‰²ååˆ—è¡¨
        
        Returns:
            åŒ…å«å„ç§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"ğŸ§  å¼€å§‹æ„å»ºç« èŠ‚{current_chapter}çš„æ™ºèƒ½ä¸Šä¸‹æ–‡...")
        
        # 1. è·å–æœ€è¿‘ç« èŠ‚ä¸Šä¸‹æ–‡(æ—¶é—´è¿ç»­æ€§)
        recent = await self.get_recent_memories(
            user_id, project_id, current_chapter, 
            recent_count=3, min_importance=0.5, db=db
        )
        
        # 2. è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        relevant = await self.search_memories(
            user_id=user_id,
            project_id=project_id,
            query=chapter_outline,
            limit=10,
            min_importance=0.4,
            db=db,
        )
        
        # 3. æŸ¥æ‰¾æœªå®Œç»“ä¼ç¬”
        foreshadows = await self.find_unresolved_foreshadows(
            user_id, project_id, current_chapter, db=db
        )
        
        # 4. å¦‚æœæœ‰æŒ‡å®šè§’è‰²,è·å–è§’è‰²ç›¸å…³è®°å¿†
        character_memories = []
        if character_names:
            character_query = " ".join(character_names) + " è§’è‰² çŠ¶æ€ å…³ç³»"
            character_memories = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query=character_query,
                memory_types=["character_event", "plot_point"],
                limit=8,
                db=db,
            )
        
        # 5. è·å–é‡è¦æƒ…èŠ‚ç‚¹
        # æ³¨æ„ï¼šChromaDBçš„whereæ¡ä»¶éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¸èƒ½åŒæ—¶ä½¿ç”¨å¤šä¸ªé¡¶å±‚æ¡ä»¶
        try:
            plot_points = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                memory_types=["plot_point", "hook"],
                limit=5,
                min_importance=0.7,
                db=db,
            )
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šåˆ†åˆ«æŸ¥è¯¢
            plot_points = []
            try:
                plot_points = await self.search_memories(
                    user_id=user_id,
                    project_id=project_id,
                    query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                    memory_types=["plot_point", "hook"],
                    limit=5,
                    db=db,
                )
            except Exception as e2:
                logger.warning(f"âš ï¸ é™çº§æŸ¥è¯¢ä¹Ÿå¤±è´¥: {str(e2)}")
                plot_points = []
        
        context = {
            "recent_context": self._format_memories(recent, "æœ€è¿‘ç« èŠ‚è®°å¿†"),
            "relevant_memories": self._format_memories(relevant, "è¯­ä¹‰ç›¸å…³è®°å¿†"),
            "character_states": self._format_memories(character_memories, "è§’è‰²ç›¸å…³è®°å¿†"),
            "foreshadows": self._format_memories(foreshadows[:5], "æœªå®Œç»“ä¼ç¬”"),
            "plot_points": self._format_memories(plot_points, "é‡è¦æƒ…èŠ‚ç‚¹"),
            "stats": {
                "recent_count": len(recent),
                "relevant_count": len(relevant),
                "character_count": len(character_memories),
                "foreshadow_count": len(foreshadows),
                "plot_point_count": len(plot_points)
            }
        }
        
        logger.info(f"âœ… ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: æœ€è¿‘{len(recent)}æ¡, ç›¸å…³{len(relevant)}æ¡, ä¼ç¬”{len(foreshadows)}ä¸ª")
        return context
    def _format_memories(self, memories: List[Dict], section_title: str = "è®°å¿†") -> str:
        """
        æ ¼å¼åŒ–è®°å¿†åˆ—è¡¨ä¸ºæ–‡æœ¬
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            section_title: ç« èŠ‚æ ‡é¢˜
        
        Returns:
            æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not memories:
            return f"ã€{section_title}ã€‘\næš‚æ— ç›¸å…³è®°å¿†\n"
        
        lines = [f"ã€{section_title}ã€‘"]
        for i, mem in enumerate(memories, 1):
            meta = mem.get('metadata', {})
            chapter_num = meta.get('chapter_number', '?')
            mem_type = meta.get('memory_type', 'æœªçŸ¥')
            importance = float(meta.get('importance', 0.5))
            title = meta.get('title', '')
            content = mem['content']
            
            # æ ¼å¼: [åºå·] ç¬¬Xç« -ç±»å‹(é‡è¦æ€§) æ ‡é¢˜: å†…å®¹
            line = f"{i}. [ç¬¬{chapter_num}ç« -{mem_type}â˜…{importance:.1f}]"
            if title:
                line += f" {title}: {content[:100]}"
            else:
                line += f" {content[:150]}"
            lines.append(line)
        
        return "\n".join(lines) + "\n"
    
    async def delete_chapter_memories(
        self,
        user_id: str,
        project_id: str,
        chapter_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šç« èŠ‚çš„æ‰€æœ‰è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            chapter_id: ç« èŠ‚ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # è¿œç«¯/local embedding å¯èƒ½å¯¹åº”ä¸åŒ collectionï¼Œè¿™é‡Œç»Ÿä¸€æ¸…ç†
            deleted_total = 0
            for name in self._list_project_collection_names(user_id, project_id):
                try:
                    collection = self.client.get_collection(name=name)
                except Exception:
                    continue

                results = collection.get(where={"chapter_id": chapter_id})
                if results and results.get("ids"):
                    collection.delete(ids=results["ids"])
                    deleted_total += len(results["ids"])

            if deleted_total > 0:
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ç« èŠ‚{chapter_id[:8]}çš„{deleted_total}æ¡å‘é‡è®°å¿†ï¼ˆè·¨collectionï¼‰")
            else:
                logger.info(f"â„¹ï¸ ç« èŠ‚{chapter_id[:8]}æ²¡æœ‰å‘é‡è®°å¿†éœ€è¦åˆ é™¤")
            return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç« èŠ‚è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def delete_project_memories(
        self,
        user_id: str,
        project_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰è®°å¿†(åŒ…æ‹¬å‘é‡æ•°æ®åº“)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # åˆ é™¤æ•´ä¸ª collectionï¼ˆåŒ…å« local ä¸ remote å˜ä½“ï¼‰
            deleted = 0
            names = self._list_project_collection_names(user_id, project_id)
            for name in names:
                try:
                    self.client.delete_collection(name=name)
                    deleted += 1
                except Exception as e:
                    # collection ä¸å­˜åœ¨ä¹Ÿç®—æˆåŠŸ
                    if "does not exist" in str(e).lower():
                        continue
                    logger.warning(f"âš ï¸ åˆ é™¤collectionå¤±è´¥: {name}: {e}")

            logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤é¡¹ç›®{project_id[:8]}çš„å‘é‡æ•°æ®åº“collection: {deleted}/{len(names)} ä¸ª")
            return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é¡¹ç›®è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def update_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
        db: Optional[AsyncSession] = None,
    ) -> bool:
        """
        æ›´æ–°è®°å¿†å†…å®¹æˆ–å…ƒæ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†ID
            content: æ–°å†…å®¹(å¯é€‰)
            metadata: æ–°å…ƒæ•°æ®(å¯é€‰)
        
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))
            
            update_data = {}
            
            if content:
                # é‡æ–°ç”Ÿæˆ embeddingï¼ˆæœ¬åœ°æˆ–è¿œç«¯ï¼‰
                embedding = (await self._embed_texts([content], embed_backend))[0]
                update_data['embeddings'] = [embedding]
                update_data['documents'] = [content]
            
            if metadata:
                # å‡†å¤‡æ–°çš„å…ƒæ•°æ®
                chroma_metadata = {}
                for key, value in metadata.items():
                    if isinstance(value, (list, dict)):
                        chroma_metadata[key] = json.dumps(value, ensure_ascii=False)
                    else:
                        chroma_metadata[key] = value
                update_data['metadatas'] = [chroma_metadata]
            
            if update_data:
                collection.update(
                    ids=[memory_id],
                    **update_data
                )
                logger.info(f"âœ… è®°å¿†å·²æ›´æ–°: {memory_id[:8]}...")
                return True
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æä¾›æ›´æ–°å†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def get_memory_stats(
        self,
        user_id: str,
        project_id: str,
        db: Optional[AsyncSession] = None,
    ) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            settings, retrieval = await self._get_user_settings_and_retrieval(user_id, db)
            embed_backend = self._resolve_embedding_backend(retrieval, settings)
            collection = self.get_collection(user_id, project_id, embed_id=embed_backend.get("embed_id", "local"))
            
            # è·å–æ‰€æœ‰è®°å¿†
            all_memories = collection.get()
            
            if not all_memories['ids']:
                return {
                    "total_count": 0,
                    "by_type": {},
                    "by_chapter": {},
                    "foreshadow_count": 0
                }
            
            # ç»Ÿè®¡å„ç±»å‹æ•°é‡
            type_counts = {}
            chapter_counts = {}
            foreshadow_count = 0
            
            for i, meta in enumerate(all_memories['metadatas']):
                mem_type = meta.get('memory_type', 'unknown')
                chapter_num = meta.get('chapter_number', 0)
                is_foreshadow = meta.get('is_foreshadow', 0)
                
                type_counts[mem_type] = type_counts.get(mem_type, 0) + 1
                chapter_counts[str(chapter_num)] = chapter_counts.get(str(chapter_num), 0) + 1
                
                if is_foreshadow == 1:
                    foreshadow_count += 1
            
            stats = {
                "total_count": len(all_memories['ids']),
                "by_type": type_counts,
                "by_chapter": chapter_counts,
                "foreshadow_count": foreshadow_count,
                "foreshadow_resolved": sum(1 for m in all_memories['metadatas'] if m.get('is_foreshadow') == 2)
            }
            
            logger.info(f"ğŸ“Š è®°å¿†ç»Ÿè®¡: æ€»è®¡{stats['total_count']}æ¡, ä¼ç¬”{foreshadow_count}ä¸ª")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}


# åˆ›å»ºå…¨å±€å®ä¾‹
memory_service = MemoryService()
            
